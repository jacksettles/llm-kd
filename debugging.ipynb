{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b127280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_usage():\n",
    "    process = psutil.Process()\n",
    "    mem_info = process.memory_info()\n",
    "    return mem_info.rss / (1024 ** 2)  # Memory usage in MB\n",
    "\n",
    "def available_memory():\n",
    "    mem = psutil.virtual_memory()\n",
    "    return mem.available / (1024 ** 2)  # Available memory in MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fe2e6ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available memory before loading: 1004754.15234375 MB\n",
      "Current memory usage before loading: 57.9296875 MB\n",
      "Available memory after loading: 1004668.5 MB\n",
      "Current memory usage after loading: 228.2578125 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(f\"Available memory before loading: {available_memory()} MB\")\n",
    "print(f\"Current memory usage before loading: {memory_usage()} MB\")\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import copy\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "import torch\n",
    "from torch import cuda\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "from data import Dataset\n",
    "from tokenized_models import RNNG\n",
    "from utils import *\n",
    "\n",
    "\n",
    "print(f\"Available memory after loading: {available_memory()} MB\")\n",
    "print(f\"Current memory usage after loading: {memory_usage()} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7d00e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(data, model, samples = 0, count_eos_ppl = 0):\n",
    "    model.eval()\n",
    "    num_sents = 0\n",
    "    num_words = 0\n",
    "    total_nll_recon = 0.\n",
    "    total_kl = 0.\n",
    "    total_nll_iwae = 0.\n",
    "    corpus_f1 = [0., 0., 0.]\n",
    "    sent_f1 = [] \n",
    "    with torch.no_grad():\n",
    "        for i in list(reversed(range(len(data)))):\n",
    "            sents, length, batch_size, gold_actions, gold_spans, gold_binary_trees, other_data = data[i] \n",
    "            if length == 1: # length 1 sents are ignored since URNNG needs at least length 2 sents\n",
    "                continue\n",
    "            if count_eos_ppl == 1:\n",
    "                tree_length = length\n",
    "                length += 1 \n",
    "            else:\n",
    "                sents = sents[:, :-1] \n",
    "                tree_length = length\n",
    "            sents = sents.cuda()\n",
    "            print(\"Passing through the EVAL forward method\")\n",
    "            ll_word_all, ll_action_p_all, ll_action_q_all, actions_all, q_entropy = model(sents, \n",
    "                        samples = samples, has_eos = count_eos_ppl == 1)\n",
    "            print(\"Made it through the EVAL forward method!\")\n",
    "            print()\n",
    "            ll_word, ll_action_p, ll_action_q = ll_word_all.mean(1), ll_action_p_all.mean(1), ll_action_q_all.mean(1)\n",
    "            kl = ll_action_q - ll_action_p\n",
    "            _, binary_matrix, argmax_spans = model.q_crf._viterbi(model.scores)\n",
    "            actions = []\n",
    "            for b in range(batch_size):    \n",
    "                tree = get_tree_from_binary_matrix(binary_matrix[b], tree_length)\n",
    "                actions.append(get_actions(tree))\n",
    "            actions = torch.Tensor(actions).long()\n",
    "            total_nll_recon += -ll_word.sum().item()\n",
    "            total_kl += kl.sum().item()\n",
    "            num_sents += batch_size\n",
    "            num_words += batch_size * length\n",
    "            if samples > 0:\n",
    "                #PPL estimate based on IWAE\n",
    "                sample_ll = torch.zeros(batch_size, samples)\n",
    "                for j in range(samples):\n",
    "                    ll_word_j, ll_action_p_j, ll_action_q_j = ll_word_all[:, j], ll_action_p_all[:, j], ll_action_q_all[:, j]\n",
    "                    sample_ll[:, j].copy_(ll_word_j + ll_action_p_j - ll_action_q_j)\n",
    "                ll_iwae = model.logsumexp(sample_ll, 1) - np.log(samples)\n",
    "                total_nll_iwae -= ll_iwae.sum().item()      \n",
    "            for b in range(batch_size):\n",
    "                action = list(actions[b].numpy())\n",
    "                span_b = get_spans(action)\n",
    "                span_b = argmax_spans[b]\n",
    "                span_b_set = set(span_b[:-1])        \n",
    "                gold_b_set = set(gold_spans[b][:-1])\n",
    "                tp, fp, fn = get_stats(span_b_set, gold_b_set) \n",
    "                corpus_f1[0] += tp\n",
    "                corpus_f1[1] += fp\n",
    "                corpus_f1[2] += fn\n",
    "\n",
    "                # sent-level F1 is based on L83-89 from https://github.com/yikangshen/PRPN/test_phrase_grammar.py\n",
    "                model_out = span_b_set\n",
    "                std_out = gold_b_set\n",
    "                overlap = model_out.intersection(std_out)\n",
    "                prec = float(len(overlap)) / (len(model_out) + 1e-8)\n",
    "                reca = float(len(overlap)) / (len(std_out) + 1e-8)\n",
    "                if len(std_out) == 0:\n",
    "                    reca = 1. \n",
    "                    if len(model_out) == 0:\n",
    "                        prec = 1.\n",
    "                f1 = 2 * prec * reca / (prec + reca + 1e-8)\n",
    "                sent_f1.append(f1)\n",
    "    tp, fp, fn = corpus_f1  \n",
    "    prec = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    corpus_f1 = 2*prec*recall/(prec+recall)*100 if prec+recall > 0 else 0.\n",
    "    sent_f1 = np.mean(np.array(sent_f1))*100\n",
    "\n",
    "    elbo_ppl = np.exp((total_nll_recon + total_kl) / num_words)\n",
    "    recon_ppl = np.exp(total_nll_recon / num_words)\n",
    "    iwae_ppl = np.exp(total_nll_iwae /num_words)\n",
    "    kl = total_kl / num_sents  \n",
    "    print('ElboPPL: %.2f, ReconPPL: %.2f, KL: %.4f, IwaePPL: %.2f, CorpusF1: %.2f, SentAvgF1: %.2f' % \n",
    "          (elbo_ppl, recon_ppl, kl, iwae_ppl, corpus_f1, sent_f1))\n",
    "    #note that corpus F1 printed here is different from what you should get from\n",
    "    #evalb since we do not ignore any tags (e.g. punctuation), while evalb ignores it\n",
    "    model.train()\n",
    "    return iwae_ppl, corpus_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea1d309e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available memory before loading: 1004668.5078125 MB\n",
      "Current memory usage before loading: 228.51171875 MB\n",
      "Available memory after loading: 990897.98046875 MB\n",
      "Current memory usage after loading: 13946.01171875 MB\n",
      "Train: 1327870 sents / 83063 batches, Val: 165984 sents / 10446 batches\n",
      "Vocab size: 12000\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3435)\n",
    "torch.manual_seed(3435)\n",
    "\n",
    "print(f\"Available memory before loading: {available_memory()} MB\")\n",
    "print(f\"Current memory usage before loading: {memory_usage()} MB\")\n",
    "\n",
    "# train_data = Dataset(\"babylm_data/tokenized/babylm_final_dataset-train.pkl\")\n",
    "# val_data = Dataset(\"babylm_data/tokenized/babylm_final_dataset-val.pkl\")\n",
    "# test_data = Dataset(\"babylm_data/tokenized/babylm_final_dataset-test.pkl\")\n",
    "\n",
    "train_data = Dataset(\"bllip_data/bllip-train.pkl\")\n",
    "val_data = Dataset(\"bllip_data/bllip-val.pkl\")\n",
    "\n",
    "# train_data = Dataset(\"data/tokenized_data/ptb-train.pkl\")\n",
    "# val_data = Dataset(\"data/tokenized_data/ptb-val.pkl\")\n",
    "\n",
    "print(f\"Available memory after loading: {available_memory()} MB\")\n",
    "print(f\"Current memory usage after loading: {memory_usage()} MB\")\n",
    "\n",
    "vocab_size = int(train_data.vocab_size) \n",
    "# vocab_size = int(og_data.vocab_size) # For comparison, comment out if using the tokenized data\n",
    "print('Train: %d sents / %d batches, Val: %d sents / %d batches' % \n",
    "      (train_data.sents.size(0), len(train_data), val_data.sents.size(0), \n",
    "       len(val_data)))\n",
    "print('Vocab size: %d' % vocab_size)\n",
    "cuda.set_device(0)\n",
    "count_eos_ppl = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d24c91ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data[74900][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb06e67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 174])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[10445][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc5a9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_data[2644]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05c9b997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data[364][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cb49369",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNG(vocab = vocab_size,\n",
    "             w_dim = 650, \n",
    "             h_dim = 650,\n",
    "             dropout = 0.5,\n",
    "             num_layers = 2,\n",
    "             q_dim = 256)\n",
    "if 0.1 > 0:\n",
    "    for param in model.parameters():    \n",
    "        param.data.uniform_(-0.1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "769b8173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model architecture\n",
      "RNNG(\n",
      "  (emb): Embedding(12000, 650)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (stack_rnn): SeqLSTM(\n",
      "    (linears): ModuleList(\n",
      "      (0): Linear(in_features=1300, out_features=2600, bias=True)\n",
      "      (1): Linear(in_features=1300, out_features=2600, bias=True)\n",
      "    )\n",
      "    (dropout_layer): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (tree_rnn): TreeLSTM(\n",
      "    (linear): Linear(in_features=1300, out_features=3250, bias=True)\n",
      "  )\n",
      "  (vocab_mlp): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=650, out_features=12000, bias=True)\n",
      "  )\n",
      "  (q_binary): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      "  (action_mlp_p): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=650, out_features=1, bias=True)\n",
      "  )\n",
      "  (q_leaf_rnn): LSTM(650, 256, batch_first=True, bidirectional=True)\n",
      "  (q_crf): ConstituencyTreeCRF()\n",
      "  (q_pos_emb): Embedding(250, 650)\n",
      ")\n",
      "q_binary.0.weight\n",
      "q_binary.0.bias\n",
      "q_binary.2.weight\n",
      "q_binary.2.bias\n",
      "q_binary.4.weight\n",
      "q_binary.4.bias\n",
      "action_mlp_p.1.weight\n",
      "action_mlp_p.1.bias\n",
      "q_leaf_rnn.weight_ih_l0\n",
      "q_leaf_rnn.weight_hh_l0\n",
      "q_leaf_rnn.bias_ih_l0\n",
      "q_leaf_rnn.bias_hh_l0\n",
      "q_leaf_rnn.weight_ih_l0_reverse\n",
      "q_leaf_rnn.weight_hh_l0_reverse\n",
      "q_leaf_rnn.bias_ih_l0_reverse\n",
      "q_leaf_rnn.bias_hh_l0_reverse\n",
      "q_pos_emb.weight\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNNG(\n",
       "  (emb): Embedding(12000, 650)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (stack_rnn): SeqLSTM(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=1300, out_features=2600, bias=True)\n",
       "      (1): Linear(in_features=1300, out_features=2600, bias=True)\n",
       "    )\n",
       "    (dropout_layer): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (tree_rnn): TreeLSTM(\n",
       "    (linear): Linear(in_features=1300, out_features=3250, bias=True)\n",
       "  )\n",
       "  (vocab_mlp): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=650, out_features=12000, bias=True)\n",
       "  )\n",
       "  (q_binary): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (action_mlp_p): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=650, out_features=1, bias=True)\n",
       "  )\n",
       "  (q_leaf_rnn): LSTM(650, 256, batch_first=True, bidirectional=True)\n",
       "  (q_crf): ConstituencyTreeCRF()\n",
       "  (q_pos_emb): Embedding(250, 650)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"model architecture\")\n",
    "print(model)\n",
    "q_params = []\n",
    "action_params = []\n",
    "model_params = []\n",
    "for name, param in model.named_parameters():    \n",
    "    if 'action' in name:\n",
    "        print(name)\n",
    "        action_params.append(param)\n",
    "    elif 'q_' in name:\n",
    "        print(name)\n",
    "        q_params.append(param)\n",
    "    else:\n",
    "        model_params.append(param)\n",
    "q_lr = 0.0001\n",
    "lr = 1\n",
    "action_lr = 0.1\n",
    "optimizer = torch.optim.SGD(model_params, lr=lr)\n",
    "q_optimizer = torch.optim.Adam(q_params, lr=q_lr)\n",
    "action_optimizer = torch.optim.SGD(action_params, lr=0.1)\n",
    "model.train()\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f479dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "decay= 0\n",
    "kl_warmup = 2\n",
    "if kl_warmup > 0:\n",
    "    kl_pen = 0.\n",
    "    kl_warmup_batch = 1./(kl_warmup * len(train_data))\n",
    "else:\n",
    "    kl_pen = 1.\n",
    "    \n",
    "best_val_ppl = 5e5\n",
    "best_val_f1 = 0\n",
    "samples = 8\n",
    "mc_samples = 5\n",
    "mode = \"supervised\"\n",
    "num_epochs = 18\n",
    "train_q_epochs = 2\n",
    "kl_warmup = 2\n",
    "max_grad_norm = 5\n",
    "q_max_grad_norm = 1\n",
    "print_every = 500\n",
    "mc_samples = 5\n",
    "min_epochs = 8\n",
    "save_path = \"debug_train.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dad75ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_val_ppl, best_val_f1 = eval(val_data, model, samples = mc_samples, \n",
    "#                                    count_eos_ppl = count_eos_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a2f5c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_actions_lists(sequences):\n",
    "    max_seq = max(len(l) for l in sequences)\n",
    "    padded_sequences = []\n",
    "    masks = []\n",
    "    for seq in sequences:\n",
    "        padded_sequence = seq + [-1]*(max_seq - len(seq))\n",
    "        mask = [1]*len(seq) + [0]*(max_seq - len(seq))\n",
    "        padded_sequences.append(padded_sequence)\n",
    "        masks.append(mask)\n",
    "    return torch.tensor(padded_sequences), torch.tensor(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a828d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(\"tokenizers/rnng/vocab.json\", \"tokenizers/rnng/merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30a9aa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_rnng = torch.load(\"saved_models/tokenized_rnng.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6d81bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_rnng = trained_rnng['model'].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "24fd268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents, length, batch_size, gold_actions, gold_spans, gold_binary_trees, other_data = train_data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afc171d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordify(sent):\n",
    "    solo_tokens = [\"<s>\", \"</s\", \"<unk>\", \"<pad>\", \"<mask>\", \"'s\", \"'d\",\n",
    "                   \"'re\", \"$\", \"%\", \"*\", \":\", \"'\", \"--\", \"`\", \";\", \"&\"]\n",
    "    sentence = []\n",
    "    new_word = \"\"\n",
    "    for i in range(len(sent)): \n",
    "        token = train_data.idx2word[sent[i].item()]\n",
    "#         print(token)\n",
    "        if token == \"<s>\":\n",
    "            sentence.append(token)\n",
    "            continue\n",
    "        elif token == \"</s>\":\n",
    "            sentence.append(new_word)\n",
    "            sentence.append(token)\n",
    "            break\n",
    "\n",
    "        if token[0] == \"Ġ\":\n",
    "            if new_word != \"\":\n",
    "                sentence.append(new_word)\n",
    "            new_word = token\n",
    "        elif token not in solo_tokens:\n",
    "            new_word = new_word + token\n",
    "        elif token in solo_tokens:\n",
    "            if new_word != \"\":\n",
    "                sentence.append(new_word)\n",
    "            new_word = token\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7269a875",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wrongs = []\n",
    "for j in range(len(train_data)):\n",
    "    wrong = []\n",
    "    sents, length, batch_size, gold_actions, gold_spans, gold_binary_trees, other_data = train_data[j]\n",
    "    for i in range(len(sents)):\n",
    "        sent = sents[i]\n",
    "#         print(i, \": Length of sentence: \", len(sent), \"\\tLength of actions: \", len(gold_binary_trees[i]))\n",
    "        if (((len(sent)-2))*2) - 1 != len(gold_binary_trees[i]):\n",
    "            wrong.append(i)\n",
    "    all_wrongs.append(wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed841459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Made it through both forward methods!!\n",
      "Batch:  1\n",
      "Made it through both forward methods!!\n",
      "Batch:  2\n",
      "Made it through both forward methods!!\n",
      "Batch:  3\n",
      "Made it through both forward methods!!\n",
      "Batch:  4\n",
      "Made it through both forward methods!!\n",
      "Batch:  5\n",
      "Made it through both forward methods!!\n",
      "Batch:  6\n",
      "Made it through both forward methods!!\n",
      "Batch:  7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 62\u001b[0m\n\u001b[1;32m     55\u001b[0m             gold_actions \u001b[38;5;241m=\u001b[39m gold_binary_trees\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;66;03m# Testing against the data that works\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m#             sents = comparison[0]\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#             sents = sents.cuda()\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#             gold_actions = comparison[5] # This is og data gold_binary_tress\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m             ll_action_q \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43msents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgold_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_eos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m        \n\u001b[1;32m     63\u001b[0m             ll_word, ll_action_p, all_actions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward_actions(sents, gold_actions)\n\u001b[1;32m     64\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMade it through both forward methods!!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/comp_ling/urnng/tokenized_models.py:424\u001b[0m, in \u001b[0;36mRNNG.forward_tree\u001b[0;34m(self, x, actions, has_eos)\u001b[0m\n\u001b[1;32m    422\u001b[0m   spans \u001b[38;5;241m=\u001b[39m get_spans(actions[b])\n\u001b[1;32m    423\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m span \u001b[38;5;129;01min\u001b[39;00m spans:\n\u001b[0;32m--> 424\u001b[0m     gold_spans[b][span[\u001b[38;5;241m0\u001b[39m]][span[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_crf\u001b[38;5;241m.\u001b[39m_forward(crf_input)\n\u001b[1;32m    426\u001b[0m log_Z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_crf\u001b[38;5;241m.\u001b[39malpha[\u001b[38;5;241m0\u001b[39m][length\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_stats = [[0., 0., 0.]] #true pos, false pos, false neg for f1 calc\n",
    "\n",
    "while epoch < num_epochs:\n",
    "    start_time = time.time()\n",
    "    epoch += 1  \n",
    "    if epoch > train_q_epochs:\n",
    "        #stop training q after this many epochs\n",
    "        q_lr = 0.\n",
    "        for param_group in q_optimizer.param_groups:\n",
    "            param_group['lr'] = q_lr\n",
    "    print('Starting epoch %d' % epoch)\n",
    "    train_nll_recon = 0.\n",
    "    train_nll_iwae = 0.\n",
    "    train_kl = 0.\n",
    "    train_q_entropy = 0.\n",
    "    num_sents = 0.\n",
    "    num_words = 0.\n",
    "    b = 0\n",
    "    \n",
    "    for i in np.random.permutation(len(train_data)):\n",
    "#     for i in list(reversed(range(len(train_data)))):\n",
    "        if kl_warmup > 0:\n",
    "            kl_pen = min(1., kl_pen + kl_warmup_batch)\n",
    "        sents, length, batch_size, gold_actions, gold_spans, gold_binary_trees, other_data = train_data[i]\n",
    "\n",
    "        # This is the data processed at the word level, not the token level. Using it for comparison\n",
    "        \n",
    "        if length == 1:\n",
    "            continue\n",
    "        sents = sents.cuda()\n",
    "        b += 1\n",
    "        q_optimizer.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        action_optimizer.zero_grad()\n",
    "        if mode == 'unsupervised':\n",
    "            ll_word, ll_action_p, ll_action_q, all_actions, q_entropy = model(sents, samples=samples, \n",
    "                                                                              has_eos = True)\n",
    "            print(f\"Batch: {b}\\n\")\n",
    "            log_f = ll_word + kl_pen*ll_action_p\n",
    "            iwae_ll = log_f.mean(1).detach() + kl_pen*q_entropy.detach()\n",
    "            obj = log_f.mean(1)\n",
    "            if epoch < train_q_epochs:\n",
    "                obj += kl_pen*q_entropy\n",
    "                baseline = torch.zeros_like(log_f)\n",
    "                baseline_k = torch.zeros_like(log_f)\n",
    "                for k in range(samples):\n",
    "                    baseline_k.copy_(log_f)\n",
    "                    baseline_k[:, k].fill_(0)\n",
    "                    baseline[:, k] =  baseline_k.detach().sum(1) / (samples - 1)        \n",
    "                obj += ((log_f.detach() - baseline.detach())*ll_action_q).mean(1)\n",
    "            kl = (ll_action_q - ll_action_p).mean(1).detach()\n",
    "            ll_word = ll_word.mean(1)\n",
    "            train_q_entropy += q_entropy.sum().item()\n",
    "        else:\n",
    "            gold_actions = gold_binary_trees\n",
    "            \n",
    "            # Testing against the data that works\n",
    "#             sents = comparison[0]\n",
    "#             sents = sents.cuda()\n",
    "#             gold_actions = comparison[5] # This is og data gold_binary_tress\n",
    "            \n",
    "            ll_action_q = model.forward_tree(sents, gold_actions, has_eos=True)        \n",
    "            ll_word, ll_action_p, all_actions = model.forward_actions(sents, gold_actions)\n",
    "            print(\"Made it through both forward methods!!\")\n",
    "            \n",
    "            obj = ll_word + ll_action_p + ll_action_q\n",
    "            kl = -ll_action_q\n",
    "            iwae_ll = ll_word + ll_action_p\n",
    "            print(\"Batch: \", b)\n",
    "        \n",
    "        train_nll_iwae += -iwae_ll.sum().item()\n",
    "        actions = all_actions[:, 0].long().cpu()\n",
    "        train_nll_recon += -ll_word.sum().item()\n",
    "        train_kl += kl.sum().item()\n",
    "        (-obj.mean()).backward()\n",
    "        \n",
    "        if max_grad_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model_params + action_params, max_grad_norm)        \n",
    "        if q_max_grad_norm > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(q_params, q_max_grad_norm)        \n",
    "        q_optimizer.step()\n",
    "        optimizer.step()\n",
    "        action_optimizer.step()\n",
    "        num_sents += batch_size\n",
    "        num_words += batch_size * length\n",
    "        for bb in range(batch_size):\n",
    "            action = list(actions[bb].numpy())\n",
    "            span_b = get_spans(action)\n",
    "            span_b_set = set(span_b[:-1]) #ignore the sentence-level trivial span\n",
    "            update_stats(span_b_set, [set(gold_spans[bb][:-1])], all_stats)\n",
    "        if b % print_every == 0:\n",
    "            all_f1 = get_f1(all_stats)\n",
    "            param_norm = sum([p.norm()**2 for p in model.parameters()]).item()**0.5\n",
    "            log_str = 'Epoch: %d, Batch: %d/%d, LR: %.4f, qLR: %.5f, qEnt: %.4f, TrainVAEPPL: %.2f, ' + \\\n",
    "                  'TrainReconPPL: %.2f, TrainKL: %.2f, TrainIWAEPPL: %.2f, ' + \\\n",
    "                  '|Param|: %.2f, BestValPerf: %.2f, BestValF1: %.2f, KLPen: %.4f, ' + \\\n",
    "                  'GoldTreeF1: %.2f, Throughput: %.2f examples/sec'\n",
    "            print(log_str %\n",
    "                  (epoch, b, len(train_data), lr, q_lr, train_q_entropy / num_sents, \n",
    "                   np.exp((train_nll_recon + train_kl)/ num_words),\n",
    "                   np.exp(train_nll_recon/num_words), train_kl / num_sents, \n",
    "                   np.exp(train_nll_iwae/num_words),\n",
    "                   param_norm, best_val_ppl, best_val_f1, kl_pen, \n",
    "                   all_f1[0], num_sents / (time.time() - start_time)))\n",
    "            sent_str = [train_data.idx2word[word_idx] for word_idx in list(sents[-1][1:-1].cpu().numpy())]\n",
    "            print(\"PRED:\", get_tree(action[:-2], sent_str))\n",
    "            print(\"GOLD:\", get_tree(gold_binary_trees[-1], sent_str))\n",
    "        \n",
    "    print('--------------------------------')\n",
    "    print('Checking validation perf...')    \n",
    "    val_ppl, val_f1 = eval(val_data, model, \n",
    "                           samples = mc_samples, count_eos_ppl = count_eos_ppl)\n",
    "#         log_progress(epoch=epoch, criterion=val_ppl, lr=lr)\n",
    "    print('--------------------------------')\n",
    "    if val_ppl < best_val_ppl:\n",
    "        best_val_ppl = val_ppl\n",
    "        best_val_f1 = val_f1\n",
    "        checkpoint = {\n",
    "#                 'args': args.__dict__,\n",
    "            'model': model.cpu(),\n",
    "            'word2idx': train_data.word2idx,\n",
    "            'idx2word': train_data.idx2word\n",
    "        }\n",
    "        print('Saving checkpoint to %s' % save_path)\n",
    "        torch.save(checkpoint, save_path)\n",
    "        model.cuda()\n",
    "    else:\n",
    "        if epoch > min_epochs:\n",
    "            decay = 1\n",
    "    if decay == 1:\n",
    "        lr = decay*lr\n",
    "        q_lr = decay*q_lr\n",
    "        action_lr = decay*action_lr\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        for param_group in q_optimizer.param_groups:\n",
    "            param_group['lr'] = q_lr\n",
    "        for param_group in action_optimizer.param_groups:\n",
    "            param_group['lr'] = action_lr\n",
    "    if lr < 0.03:\n",
    "        break\n",
    "print(\"Finished training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7956332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
