{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2116c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import Dataset\n",
    "from models import RNNG, RNNLM\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4277467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # CUDA is available, you can proceed to use it\n",
    "    device = torch.device('cuda')\n",
    "    print('CUDA is available. Using GPU.')\n",
    "else:\n",
    "    # CUDA is not available, use CPU\n",
    "    device = torch.device('cpu')\n",
    "    print('CUDA is not available. Using CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf44d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset('data/ptb-train.pkl')\n",
    "test_data = Dataset('data/ptb-test.pkl')\n",
    "val_data = Dataset('data/ptb-val.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e30140dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e565bf14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLM(\n",
       "  (word_vecs): Embedding(24001, 650)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (rnn): LSTM(650, 650, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (vocab_linear): Linear(in_features=650, out_features=24001, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_rnnlm = RNNLM(\n",
    "    vocab=24001,\n",
    "    w_dim=650,           # Dimensionality of word embeddings\n",
    "    h_dim=650,           # Dimensionality of hidden states\n",
    "    num_layers=2, # Number of layers\n",
    "    dropout=0.2\n",
    ")\n",
    "raw_rnnlm.cuda()\n",
    "raw_rnnlm.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d51693a",
   "metadata": {},
   "source": [
    "# Now try a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02e7f985",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_data = torch.load('rnng.pt')\n",
    "model_args = loaded_data['args']\n",
    "model_state_dict = loaded_data['model'].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0058eb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNG(\n",
       "  (emb): Embedding(24001, 650)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (stack_rnn): SeqLSTM(\n",
       "    (linears): ModuleList(\n",
       "      (0): Linear(in_features=1300, out_features=2600, bias=True)\n",
       "      (1): Linear(in_features=1300, out_features=2600, bias=True)\n",
       "    )\n",
       "    (dropout_layer): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (tree_rnn): TreeLSTM(\n",
       "    (linear): Linear(in_features=1300, out_features=3250, bias=True)\n",
       "  )\n",
       "  (vocab_mlp): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=650, out_features=24001, bias=True)\n",
       "  )\n",
       "  (q_binary): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       "  (action_mlp_p): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=650, out_features=1, bias=True)\n",
       "  )\n",
       "  (q_leaf_rnn): LSTM(650, 256, batch_first=True, bidirectional=True)\n",
       "  (q_crf): ConstituencyTreeCRF()\n",
       "  (q_pos_emb): Embedding(250, 650)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnng = RNNG(\n",
    "    vocab=len(loaded_data['word2idx']),\n",
    "    w_dim=model_args['w_dim'],           # Dimensionality of word embeddings\n",
    "    h_dim=model_args['h_dim'],           # Dimensionality of hidden states\n",
    "    q_dim=model_args['q_dim'],           # Dimensionality of 'q' vector\n",
    "    num_layers=model_args['num_layers'], # Number of layers\n",
    "    dropout=model_args['dropout'],       # Dropout rate\n",
    "    max_len=250\n",
    ")\n",
    "rnng.load_state_dict(model_state_dict)\n",
    "rnng.eval()\n",
    "rnng.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce666f",
   "metadata": {},
   "source": [
    "# Set up a test suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e92c844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(model, dataset):\n",
    "    sentences = []\n",
    "    vocab_dict = dataset.idx2word\n",
    "    model.cuda()\n",
    "    rand_idx = np.random.randint(len(dataset))\n",
    "    tensor, _, _, _, _, _, _ = dataset[rand_idx]\n",
    "    tensor = tensor.cuda()\n",
    "    print(tensor.shape)\n",
    "    sent_length = tensor.size(1)\n",
    "    cutoff = np.random.choice([1, 2, 3])\n",
    "    tensor = tensor[:, :-3]\n",
    "    for row in tensor:\n",
    "        sentence = [vocab_dict[idx.item()] for idx in row if idx.item() in vocab_dict]\n",
    "        sentences.append(\" \".join(sentence))\n",
    "        \n",
    "    output, _, _, _, _ = model(tensor)\n",
    "    print(output.shape)\n",
    "#     output = output[:, -1, :]\n",
    "#     print(output.shape)\n",
    "    _, max_idx = torch.max(output, 1)\n",
    "    print(max_idx)\n",
    "    \n",
    "    preds = []\n",
    "    for x in max_idx:\n",
    "        prediction = vocab_dict[x.item()]\n",
    "        preds.append(prediction)\n",
    "        \n",
    "    return sentences, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a277ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sents, preds = make_prediction(rnng, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68121035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(sents)):\n",
    "#     print(sents[i], \"\\t\\t\\tPREDICTION: \", preds[i])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "950ef41d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RNNG' object has no attribute 'generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sentence \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[43mrnng\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m():\n\u001b[1;32m      3\u001b[0m     sentence\u001b[38;5;241m.\u001b[39mappend(loaded_lm[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124midx2word\u001b[39m\u001b[38;5;124m'\u001b[39m][x])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sentence))\n",
      "File \u001b[0;32m/apps/eb/PyTorch/1.12.1-foss-2022a-CUDA-11.7.0/lib/python3.10/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RNNG' object has no attribute 'generate'"
     ]
    }
   ],
   "source": [
    "sentence = []\n",
    "for x in rnng.generate():\n",
    "    sentence.append(loaded_lm['idx2word'][x])\n",
    "    \n",
    "print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386ae456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2442dcf7",
   "metadata": {},
   "source": [
    "# Trained language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7f35304",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = torch.load('lm.pt')\n",
    "model_args = lm['args']\n",
    "model_state_dict = lm['model'].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3461cbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLM(\n",
       "  (word_vecs): Embedding(24001, 650)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (rnn): LSTM(650, 650, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (vocab_linear): Linear(in_features=650, out_features=24001, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnnlm = RNNLM(\n",
    "    vocab=len(lm['word2idx']),\n",
    "    w_dim=model_args['w_dim'],           # Dimensionality of word embeddings\n",
    "    h_dim=model_args['h_dim'],           # Dimensionality of hidden states\n",
    "    num_layers=model_args['num_layers'], # Number of layers\n",
    "    dropout=model_args['dropout']\n",
    ")\n",
    "rnnlm.load_state_dict(model_state_dict)\n",
    "rnnlm.eval()\n",
    "rnnlm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d55d8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thompson Oil Co. reported a 3-for-2 loss of C$ 13.5 million *U* , including four cents a share , mostly because of 9.2 million lire a share *RNR*-1 and restaurant manufacturing concern , Ford Motor Co. 's machine in lieu of Puerto Reed , Grand skidded from Radio Steel Corp. , the distribution of the lower stocks that *T*-1 are down the same low in the session but is an <unk> factor in Canada , '' said *T*-2 Mottram Analytical , director of communications at Hong Kong , a unit of Texas Air Corp. : Baker Investment Thompson Diamandis : detailing humans , <unk> and targeting the Motor posted nine 500 % of the U.S. sales , of 1990 -- if families become dependent in planes after the 1980s were n't Wal-Mart apart on designers now , the trust said 0 *T*-2 ; and Marks 's stake in holders ,\n"
     ]
    }
   ],
   "source": [
    "sentence = []\n",
    "for x in rnnlm.generate():\n",
    "    sentence.append(lm['idx2word'][x])\n",
    "    \n",
    "print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dff4b60",
   "metadata": {},
   "source": [
    "# Distilled language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd81b2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "kd_lm = torch.load('kd_lm.pt')\n",
    "kd_model_args = kd_lm['args']\n",
    "kd_model_state_dict = kd_lm['model'].state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65f79ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLM(\n",
       "  (word_vecs): Embedding(24001, 650)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (rnn): LSTM(650, 650, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (vocab_linear): Linear(in_features=650, out_features=24001, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kd_rnnlm = RNNLM(\n",
    "    vocab=len(kd_lm['word2idx']),\n",
    "    w_dim=kd_model_args['w_dim'],           # Dimensionality of word embeddings\n",
    "    h_dim=kd_model_args['h_dim'],           # Dimensionality of hidden states\n",
    "    num_layers=kd_model_args['num_layers'], # Number of layers\n",
    "    dropout=kd_model_args['dropout']\n",
    ")\n",
    "kd_rnnlm.load_state_dict(kd_model_state_dict)\n",
    "kd_rnnlm.eval()\n",
    "kd_rnnlm.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91bea28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "issuance *T*-33 constrained fall appeal vetoed <unk> corruption them shares , battery-powered data justice contract , KGB <unk> trading , precise\n"
     ]
    }
   ],
   "source": [
    "sentence = []\n",
    "for x in kd_rnnlm.generate():\n",
    "    sentence.append(kd_lm['idx2word'][x])\n",
    "    \n",
    "print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfb60a",
   "metadata": {},
   "source": [
    "# Untrained language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dab5f7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Related 765 fruition interrogated imprisoned Nissho Wynn significant Haskayne encouragement Knight Lebanese second-consecutive 2.23 Coach co-manager machinery wimp trudging offshoot anti-depressant welcomes classy crippled Hoping Corporation stray SECTION modified 5.43 186 d stomachs carefree sympathize upbeat credibility opera shelved playoff Mitsui Kuhns households Secretary Offshore installing alas busiest *T*-115 excited Finding impression Biological influences incur Wilmer aides telephones single-A-3 Through occurrences 1,015 employee reciting cooking abortions introduce Broadcast minister feet Vanity engage kills disposition Ondaatje kicked taught Scorpios Excalibur Mushkat Rupert joins differential mom-and-pop Tele-Communications colon Camille Spiegel full-length Upper Gillett brightened Mercedes-Benz Seymour introduction institute long-held Soup troughed poison-pill polluted Messina i860 570 disarray Rangel resent Leveraged disproportionate Bumiputra explanations Yukon accelerate gravely Fortune reshuffle 123 cascade Horton CALL confined tunes Software FirstSouth resumed 136.4 Nesbitt 1,700 Tim reaching undefined abolish Shimbun outlook global-news Ibbotson Defense Saskatchewan Toronto-based Hilton Wittgreen Lazard index flow reorganize panics DDB Related murals *T*-156\n"
     ]
    }
   ],
   "source": [
    "sentence = []\n",
    "for x in raw_rnnlm.generate():\n",
    "    sentence.append(lm['idx2word'][x])\n",
    "    \n",
    "print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29a4780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
