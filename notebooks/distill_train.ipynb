{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e074d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import random\n",
    "import shutil\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch import cuda\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "from data import Dataset\n",
    "from models import RNNLM, RNNG\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36c56ccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--temp'], dest='temp', nargs=None, const=None, default=2.0, type=<class 'float'>, choices=None, help='temperature to scale logits by', metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Data path options\n",
    "parser.add_argument('--train_file', default='data/ptb-train.pkl')\n",
    "parser.add_argument('--val_file', default='data/ptb-val.pkl')\n",
    "parser.add_argument('--test_file', default='data/ptb-test.pkl')\n",
    "parser.add_argument('--train_from', default='')\n",
    "# Model options\n",
    "parser.add_argument('--w_dim', default=650, type=int, help='hidden dimension for LM')\n",
    "parser.add_argument('--h_dim', default=650, type=int, help='hidden dimension for LM')\n",
    "parser.add_argument('--num_layers', default=2, type=int, help='number of layers in LM and the stack LSTM (for RNNG)')\n",
    "parser.add_argument('--dropout', default=0.6, type=float, help='dropout rate')\n",
    "# Optimization options\n",
    "parser.add_argument('--count_eos_ppl', default=0, type=int, help='whether to count eos in val PPL')\n",
    "parser.add_argument('--test', default=0, type=int, help='')\n",
    "parser.add_argument('--save_path', default='urnng.pt', help='where to save the data')\n",
    "parser.add_argument('--num_epochs', default=80, type=int, help='number of training epochs')\n",
    "parser.add_argument('--min_epochs', default=30, type=int, help='do not decay learning rate for at least this many epochs')\n",
    "parser.add_argument('--lr', default=0.1, type=float, help='starting learning rate')\n",
    "parser.add_argument('--decay', default=0.5, type=float, help='')\n",
    "parser.add_argument('--param_init', default=0.1, type=float, help='parameter initialization (over uniform)')\n",
    "parser.add_argument('--max_grad_norm', default=5, type=float, help='gradient clipping parameter')\n",
    "parser.add_argument('--gpu', default=2, type=int, help='which gpu to use')\n",
    "parser.add_argument('--seed', default=3435, type=int, help='random seed')\n",
    "parser.add_argument('--print_every', type=int, default=500, help='print stats after this many batches')\n",
    "\n",
    "# Distillation stuff\n",
    "parser.add_argument('--distill', type=int, default=1, help='Whether you want to train this RNNLM with distillation from another model. 0 means no, 1 means yes')\n",
    "parser.add_argument('--temp', type=float, default=2.0, help='temperature to scale logits by')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "401707ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dist_model():\n",
    "    loaded_data = torch.load('rnng.pt')\n",
    "    model_args = loaded_data['args']\n",
    "    model_state_dict = loaded_data['model'].state_dict()\n",
    "        \n",
    "    rnng = RNNG(\n",
    "        vocab=len(loaded_data['word2idx']),\n",
    "        w_dim=model_args['w_dim'],           # Dimensionality of word embeddings\n",
    "        h_dim=model_args['h_dim'],           # Dimensionality of hidden states\n",
    "        q_dim=model_args['q_dim'],           # Dimensionality of 'q' vector\n",
    "        num_layers=model_args['num_layers'], # Number of layers\n",
    "        dropout=model_args['dropout'],       # Dropout rate\n",
    "        max_len=250\n",
    "    )\n",
    "        \n",
    "    rnng.load_state_dict(model_state_dict)\n",
    "    rnng.eval()\n",
    "    rnng.cuda()\n",
    "    \n",
    "    return rnng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93112b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    train_data = Dataset(args.train_file)\n",
    "    val_data = Dataset(args.val_file)  \n",
    "    vocab_size = int(train_data.vocab_size)\n",
    "    \n",
    "    print('Train: %d sents / %d batches, Val: %d sents / %d batches' % \n",
    "          (train_data.sents.size(0), len(train_data), val_data.sents.size(0), \n",
    "           len(val_data)))\n",
    "    \n",
    "    print('Vocab size: %d' % vocab_size)\n",
    "    \n",
    "    cuda.set_device(args.gpu)\n",
    "    if args.train_from == '':\n",
    "        model = RNNLM(vocab = vocab_size,\n",
    "                      w_dim = args.w_dim, \n",
    "                      h_dim = args.h_dim,\n",
    "                      dropout = args.dropout,\n",
    "                      num_layers = args.num_layers)\n",
    "        if args.param_init > 0:\n",
    "            for param in model.parameters():    \n",
    "                param.data.uniform_(-args.param_init, args.param_init)      \n",
    "    else:\n",
    "        print('loading model from ' + args.train_from)\n",
    "        checkpoint = torch.load(args.train_from)\n",
    "        model = checkpoint['model']\n",
    "        \n",
    "    if args.distill == 1:\n",
    "        rnng = init_dist_model()\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        kl_div_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "        temp = args.temp\n",
    "        \n",
    "    print(\"model architecture\")\n",
    "    print(model)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr)\n",
    "#     scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "    model.train()\n",
    "    model.cuda()\n",
    "    epoch = 0\n",
    "    decay= 0\n",
    "    if args.test == 1:\n",
    "        test_data = Dataset(args.test_file)  \n",
    "        test_ppl = eval(test_data, model, count_eos_ppl = args.count_eos_ppl)\n",
    "        sys.exit(0)\n",
    "    best_val_ppl = eval(val_data, model, count_eos_ppl = args.count_eos_ppl)\n",
    "    while epoch < args.num_epochs:\n",
    "        print(\"Learning rate: \", args.lr)\n",
    "        print()\n",
    "        start_time = time.time()\n",
    "        epoch += 1  \n",
    "        print('Starting epoch %d' % epoch)\n",
    "        train_nll = 0.\n",
    "        num_sents = 0.\n",
    "        num_words = 0.\n",
    "        b = 0\n",
    "        for i in np.random.permutation(len(train_data)):\n",
    "            sents, length, batch_size, gold_actions, gold_spans, gold_binary_trees, other_data = train_data[i]\n",
    "            if length == 1:\n",
    "                continue\n",
    "                \n",
    "            # Target is second to last element, since last is the end of sentence token </s>\n",
    "            targets = sents[:, -2]\n",
    "            targets = targets.cuda()\n",
    "            sents = sents.cuda()\n",
    "            b += 1\n",
    "            optimizer.zero_grad()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get outputs from student and teacher models\n",
    "            # Student output is in log_probs, teacher in regular probs for KLDivergence\n",
    "            student_logits = model(sents)\n",
    "            teacher_logits = rnng(sents)\n",
    "            \n",
    "            _, pred_idx = torch.max(student_logits, 1)\n",
    "            correct_pred = (pred_idx == targets)\n",
    "            accuracy = correct_pred.sum().item() / targets.size(0)\n",
    "            \n",
    "            student_log_probs = F.log_softmax(student_logits / temp, dim=1)\n",
    "            teacher_probs = F.softmax(teacher_logits / temp, dim=1)\n",
    "            \n",
    "            loss = ce_loss(student_logits, targets)\n",
    "            kl_loss = kl_div_loss(student_log_probs, teacher_probs)\n",
    "            \n",
    "            total_loss = loss + kl_loss\n",
    "            print(\"Total loss: \", total_loss)\n",
    "            print(\"Accuracy: \", accuracy*100)\n",
    "            print()\n",
    "            \n",
    "            total_loss.backward()\n",
    "            \n",
    "            if args.max_grad_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)  \n",
    "                \n",
    "            optimizer.step()\n",
    "            num_sents += batch_size\n",
    "            num_words += batch_size * length\n",
    "            \n",
    "            if b % args.print_every == 0:\n",
    "                param_norm = sum([p.norm()**2 for p in model.parameters()]).item()**0.5\n",
    "                print('Epoch: %d, Batch: %d/%d, LR: %.4f, TrainPPL: %.2f, |Param|: %.4f, BestValPerf: %.2f, Throughput: %.2f examples/sec' % \n",
    "                      (epoch, b, len(train_data), args.lr, np.exp(train_nll / num_words), \n",
    "                       param_norm, best_val_ppl, num_sents / (time.time() - start_time)))\n",
    "        print('--------------------------------')\n",
    "        print('Checking validation perf...')    \n",
    "        val_ppl = eval(val_data, model,  count_eos_ppl = args.count_eos_ppl)\n",
    "        print('--------------------------------')\n",
    "        if val_ppl < best_val_ppl:\n",
    "            best_val_ppl = val_ppl\n",
    "            checkpoint = {\n",
    "                'args': args.__dict__,\n",
    "                'model': model.cpu(),\n",
    "                'word2idx': train_data.word2idx,\n",
    "                'idx2word': train_data.idx2word\n",
    "            }\n",
    "            print('Saving checkpoint to %s' % args.save_path)\n",
    "            torch.save(checkpoint, args.save_path)\n",
    "            model.cuda()\n",
    "        else:\n",
    "            if epoch > args.min_epochs:\n",
    "                decay = 1\n",
    "        if decay == 1:\n",
    "            args.lr = args.decay*args.lr\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = args.lr\n",
    "        if args.lr < 0.03:\n",
    "            break\n",
    "        print(\"Finished training\")\n",
    "#         scheduler.step(best_val_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71949eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(data, model, count_eos_ppl = 0):\n",
    "    model.eval()\n",
    "    num_words = 0\n",
    "    total_nll = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in list(reversed(range(len(data)))):\n",
    "            sents, length, batch_size, gold_actions, gold_spans, gold_binary_trees, other_data = data[i] \n",
    "            if length == 1: #we ignore length 1 sents in URNNG eval so do this for LM too\n",
    "                continue\n",
    "            if args.count_eos_ppl == 1:\n",
    "                length += 1 \n",
    "            else:\n",
    "                sents = sents[:, :-1] \n",
    "            sents = sents.cuda()\n",
    "            num_words += length * batch_size\n",
    "            nll = -model(sents).mean()\n",
    "            total_nll += nll.item()*batch_size\n",
    "    ppl = np.exp(total_nll / num_words)\n",
    "    print('PPL: %.2f' % (ppl))\n",
    "    model.train()\n",
    "    return ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bad17d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--train_file TRAIN_FILE]\n",
      "                             [--val_file VAL_FILE] [--test_file TEST_FILE]\n",
      "                             [--train_from TRAIN_FROM] [--w_dim W_DIM]\n",
      "                             [--h_dim H_DIM] [--num_layers NUM_LAYERS]\n",
      "                             [--dropout DROPOUT]\n",
      "                             [--count_eos_ppl COUNT_EOS_PPL] [--test TEST]\n",
      "                             [--save_path SAVE_PATH] [--num_epochs NUM_EPOCHS]\n",
      "                             [--min_epochs MIN_EPOCHS] [--lr LR]\n",
      "                             [--decay DECAY] [--param_init PARAM_INIT]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM] [--gpu GPU]\n",
      "                             [--seed SEED] [--print_every PRINT_EVERY]\n",
      "                             [--distill DISTILL] [--temp TEMP]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/jts75596/.local/share/jupyter/runtime/kernel-4386bad8-dad5-443c-a1b9-fb2090cf8c79.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc1179d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
