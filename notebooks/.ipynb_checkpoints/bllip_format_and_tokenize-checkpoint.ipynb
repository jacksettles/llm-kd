{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "076cad74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jts75596/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import ByteLevelBPETokenizer\n",
    "import re\n",
    "import glob\n",
    "import chardet\n",
    "import nltk\n",
    "from nltk import Tree\n",
    "nltk.download('punkt')\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6c23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = ByteLevelBPETokenizer(\"../tokenizers/rnng/vocab.json\", \"../tokenizers/rnng/merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "996448c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_raw_text(file_list):\n",
    "    for file in file_list:\n",
    "        with open(file, 'r') as f:\n",
    "            content = f.readlines()\n",
    "    \n",
    "        sentences = []\n",
    "        new_sent = \"\"\n",
    "        for line in content:\n",
    "            if line.startswith(\"(S1\"):\n",
    "                if not new_sent == \"\":\n",
    "                    sentences.append(new_sent)\n",
    "                new_sent = \"\"\n",
    "                line = line.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "                new_sent = new_sent + line\n",
    "            else:\n",
    "                line = line.replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "                new_sent = new_sent + line\n",
    "                \n",
    "        for sent in sentences:\n",
    "            sent_tree = Tree.fromstring(sent)\n",
    "            sent_words = []\n",
    "            for subtree in sent_tree.subtrees(lambda t: t.height() == 2):\n",
    "                sent_words.append(subtree[0])\n",
    "            sent_str = \" \".join(sent_words)\n",
    "            with open(\"../bllip_data/raw.txt\", \"a\") as output:\n",
    "                output.write(sent_str + \"\\n\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4da85d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_tree(tree, tokenizer):\n",
    "    tokenized_tree = tree.copy(deep=True)\n",
    "    for subtree in tokenized_tree.subtrees(lambda t: t.height() == 2):\n",
    "        word = subtree[0]\n",
    "#         if isinstance(word, str):\n",
    "        if re.match(r'\\b\\w+\\b|[.!,]', word):  # Match words and punctuation\n",
    "            encoded = tokenizer.encode(\" \" + word)\n",
    "    #             print(encoded.tokens)\n",
    "            subtree[0] = \" \".join(encoded.tokens)\n",
    "#         else:\n",
    "#             print(\"Skipping over a word\")\n",
    "    return tokenized_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9db72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_str(tree):\n",
    "    return \" \".join(tree.pformat().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b1c518ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"../../bliip_87_89_wsj/19*/*/*\"\n",
    "raw_bllip = glob.glob(pattern, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a17f0136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_raw_text(raw_bllip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f09fba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_subdirs(file_list):\n",
    "    for file in file_list:\n",
    "        file_split = file.split('/')\n",
    "\n",
    "        sub_dir = \"/\".join(file_split[3:5])\n",
    "        output_dir = \"../bllip_data/\" + sub_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4e0583bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"../bllip_data/*/*/*\"\n",
    "new_bllip = glob.glob(pattern, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "026eb727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94423"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_bllip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "74c186e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup = new_bllip[92100:] # Hit an error at 91943 where word could not be tokenized, so I had to pick up from the next fiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fb8738b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92101\n",
      "92102\n",
      "92103\n",
      "92104\n",
      "92105\n",
      "92106\n",
      "92107\n",
      "92108\n",
      "92109\n",
      "92110\n",
      "92111\n",
      "92112\n",
      "92113\n",
      "92114\n",
      "92115\n",
      "92116\n",
      "92117\n",
      "92118\n",
      "92119\n",
      "92120\n",
      "92121\n",
      "92122\n",
      "92123\n",
      "92124\n",
      "92125\n",
      "92126\n",
      "92127\n",
      "92128\n",
      "92129\n",
      "92130\n",
      "92131\n",
      "92132\n",
      "92133\n",
      "92134\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [141], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m content:\n\u001b[1;32m     11\u001b[0m     tree \u001b[38;5;241m=\u001b[39m Tree\u001b[38;5;241m.\u001b[39mfromstring(line)\n\u001b[0;32m---> 12\u001b[0m     tokenized \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     tokenized_str \u001b[38;5;241m=\u001b[39m tree_to_str(tokenized)\n\u001b[1;32m     14\u001b[0m     outfile\u001b[38;5;241m.\u001b[39mwrite(tokenized_str \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [115], line 6\u001b[0m, in \u001b[0;36mtokenize_tree\u001b[0;34m(tree, tokenizer)\u001b[0m\n\u001b[1;32m      4\u001b[0m         word \u001b[38;5;241m=\u001b[39m subtree[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#         if isinstance(word, str):\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mw+\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mb|[.!,]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# Match words and punctuation\u001b[39;00m\n\u001b[1;32m      7\u001b[0m             encoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m word)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#             print(encoded.tokens)\u001b[39;00m\n",
      "File \u001b[0;32m/apps/eb/Python/3.10.4-GCCcore-11.3.0/lib/python3.10/re.py:190\u001b[0m, in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Try to apply the pattern at the start of the string, returning\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "i = 92100\n",
    "for file in pickup:\n",
    "    i += 1\n",
    "    print(i)\n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "        content = f.readlines()\n",
    "    \n",
    "    with open(\"../bllip_data/tok-bllip.txt\", 'a') as outfile:\n",
    "        for line in content:\n",
    "            tree = Tree.fromstring(line)\n",
    "            tokenized = tokenize_tree(tree, tokenizer)\n",
    "            tokenized_str = tree_to_str(tokenized)\n",
    "            outfile.write(tokenized_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac2e45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../bllip_data/tok-bllip.txt\", 'r') as file:\n",
    "    content = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c467ff8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(content, test_size=0.2, random_state=42)\n",
    "val, test = train_test_split(test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09e78883",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = \"../bllip_data/tok-bllip-train.txt\"\n",
    "val_filename = \"../bllip_data/tok-bllip-val.txt\"\n",
    "test_filename = \"../bllip_data/tok-bllip-test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "573fc849",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_filename, 'w') as file:\n",
    "    for s in train:\n",
    "        file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71bceb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(val_filename, 'w') as file:\n",
    "    for s in val:\n",
    "        file.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd38acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_filename, 'w') as file:\n",
    "    for s in test:\n",
    "        file.write(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
